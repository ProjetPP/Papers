\section{Machine Learning: Window approach}

The goal of this module is to make an alternative to the grammatical approach, to product triples from English sentences.

We used machine learning algorithms in order to produce triples from scratch, i.e. without any grammatical library like \Stanford.


Motivations come from three points:
\begin{itemize}
\item Because triples are linked to the semantic of the sentence, and not directly from the grammar, we could except that avoid grammatical tools can be a good idea.
\item It has been shown that a machine learning approach can produce, for a large panel of different NLP problems, very good solutions, closed to \textit{state of the art} algorithms \cite{collobert}.
\item Grammatical approaches fail on keyword sentences, like "Barack Obama birth date" because these kind of sentences does not respect the syntax of English.
\end{itemize}

This work is based mainly on the paper "Natural Language Processing (almost) from Scratch" \cite{collobert}.

Due to the restricted amount of time, we emphasis on keywords questions (keywords questions can't be analysis with grammatical tools) and we limit ourself to a restricted version of the data model:
\begin{itemize}
\item Only one level of depth: for example the sentence "What is the birth date of the president of the United States?" will be converted to the triple: \hl{(president of the United States, birth date, ?)}. 
\item We do not support \textit{types}, and connectors like \textit{first}, \textit{sort}...
\end{itemize}
We used a look-up table and a window approach neural network as explain below. The complete package\footnote{\url{https://github.com/ProjetPP/PPP-NLP-ML-standalone/}} was written in Python 3. We use the scikit-learn library, numpy and nltk as external tool.

Our algorithm can be described with three composants: the look up table, the window approach, and the classifier.

\subsubsection{Look-up table}

The look-up table is a dictionary that associates to each word $w$ a vector $V_w \in \mathbb{R}^n$, where n is the number of parameters used to encode a word (we used $n=25$).
If two English words $w_1$ and $w_2$ are synonymous, then $||w_1-w_2||_2$ is small.

The construction of the look-up table is described in \cite{collobert} and used unsupervised machine learning techniques.
We used the precomputed look-up table found here: \url{http://metaoptimize.com/projects/wordreprs/}

We also add one parameter to know if the word starts with a capitalize character or not. Finally words are embedded in vectors of dimension 26. 

\subsubsection{Window approach}

We used a window (as explain in \cite{collobert}) that focuses on one word to classify. For example, if the sentence is "What is the birth date of the president of France?", and the word to classify is "date", for a window size of 7, the window is: "is the birth \textbf{date} of the president".


We used this window because classifier algorithms usually work with a fixed number of input parameters. 

The window size is a meta parameter to choose. This window has to be large enough that we can decide in the context of the window in which category a word is. We used a window of size 9.

\subsubsection{The classifier}

If the question is "What is the birth date of the president of France?", and the focus of the window approch is on the word \textbf{date}, then the goal is to classify the word \textbf{date} into one of these four categories: \textit{subject}, \textit{predicate}, \textit{object}, \textit{to forget}.

The classification of each word of the sentence into these four categories finally give us the desired triple.

The principe of the complete algorithm can be summarize in this figure: 

\begin{figure}[!ht]
  \centering
  \caption{The architecture of the algorithm, as described in \cite{collobert}}
  \label{sandalone:model}
    \includegraphics[scale=0.5]{../NLP-standalone-images/model.png}
\end{figure}

Because we have a a small data set of annotated questions (see below), we used a linear model: a linear model has the advantage to have few parameters.
For example with a window approach of size 9 and words embedded in vectors of dimension 26,  this gives us $26\times 9\times 4 = 936$ parameters to learn.

In a first time we implemented our own linear classifier, but to improve the execution time, we finally used the \textit{LDA} classifier from the library scikit-learn. Few seconds of computation are now needed to train successfully the model.

\subsection{Data set}

Because we used supervised algorithms, we need a data set of annotated questions, in order to learn our model.
This data set was mostly build manually, because we did not find on internet a data set that directly answering the problem of triple extraction.
Build this data set is a fastidious work. Currently our data set is composed of 300 questions.
We also wrote a script that generate keywords questions. This script give us 500 keywords questions and make the module specialized on keywords questions.

We now denote $\mathcal{D}_{GC}$ (for \textit{Dataset Grammaticaly Correct}) the data set of grammatically correct questions, $\mathcal{D}_{kw}$ for the data set of generated keywords questions and $\mathcal{D}_A = \mathcal{D}_{GC} \cup  \mathcal{D}_{KW}$ the complete data set.

\subsection{Results}

In this basic configuration, we can measure the accurency of our algorithm, defined as the ratio of correctly classified words, for a specified data set.

Our methodology is the following: for a specified data set $\mathcal{D} \in \{\mathcal{D}_A, \mathcal{D}_{GC}, \mathcal{D}_{KW}\}$ we split $\mathcal{D}$ in two parts: the training set and the test set (90% of the data is for the training set and 10% for the testing set). We learn our classifier with the training set and then evaluate it on the test set.

\textbf{Remark}: to compute the accurency of one variant of our algorithm we split the data set into testing and training set randomly, and we restart an experience 50 times. This make us sure of the precision of the estimated accurency.

\begin{center}
\begin{tabular}{|l|c|r|}
  \hline
  Data set & Training accurency  & Testing accurency \\
  \hline
  $\mathcal{D}_{GC}$ & $8\%$ & $75\%$ \\
  $\mathcal{D}_{KW}$ & $98\%$ & $98.3\%$ \\
  $\mathcal{D}_{A}$    & $83\%$ & $86\%$ \\
  \hline
\end{tabular}
\end{center}

We can conclude that this version of the algorithm is excellent for keyword questions, and not really good for grammatically correct questions.

\subsubsection{Bias vs Variance test}

We can plot the \textit{Bias vs Variance} curves:

\begin{figure}[!ht]
  \centering
  \caption{Bias vs Variance curve for the data set  $\mathcal{D}_{GC}$. This curve show us that there is a small gap between the two performences curves: the data set of grammaticaly correct questions is surely too small}
  \label{sandalone:bias_vs_variance_1}
    \includegraphics[scale=0.5]{../NLP-standalone-images/BiasVsVarianceD_GC.png}
\end{figure}

\begin{figure}[!ht]
  \centering
  \caption{Bias vs Variance curve for the data set  $\mathcal{D}_{KW}$. This curve show us that there is no gap between the two performences curves: the data set of keywords questions is enougth larger, and the performances are good.}
  \label{sandalone:bias_vs_variance_2}
    \includegraphics[scale=0.5]{../NLP-standalone-images/BiasVsVarianceD_KW.png}
\end{figure}

\begin{figure}[!ht]
  \centering
  \caption{Bias vs Variance curve for the data set  $\mathcal{D}_{A}$. }
  \label{sandalone:bias_vs_variance_3}
    \includegraphics[scale=0.5]{../NLP-standalone-images/BiasVsVarianceD_A.png}
\end{figure}


